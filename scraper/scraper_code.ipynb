{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#tidies extracted text \\ndef process_bio(bio):\\n    bio = bio.encode(\\'ascii\\',errors=\\'ignore\\').decode(\\'utf-8\\')       #removes non-ascii characters\\n    bio = re.sub(\\'\\\\s+\\',\\' \\',bio)       #repalces repeated whitespace characters with single space\\n    return bio\\n\\ndef remove_script(soup):\\n    for script in soup([\"script\", \"style\"]):\\n        script.decompose()\\n    return soup\\n\\n\\n#Checks if bio_url is valid \\ndef is_valid_homepage(bio_url,dir_url):\\n    try:\\n        ret_url = urllib.request.urlopen(bio_url).geturl() \\n    except:\\n        return False\\n    urls = [re.sub(\\'((https?://)|(www.))\\',\\'\\',url) for url in [ret_url,dir_url]] #removes url scheme (https,http) or www \\n    return not(urls[0]== urls[1])\\n    \\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import re \n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "import time\n",
    "import random\n",
    "\n",
    "def writew(lst,file_):\n",
    "    with open(file_,'w') as f:\n",
    "        for l in lst:\n",
    "            f.write(l)\n",
    "            f.write('\\n')\n",
    "    \n",
    "def writea(lst,file_):\n",
    "    with open(file_,'a') as f:\n",
    "        for l in lst:\n",
    "            f.write(l)\n",
    "            f.write('\\n')\n",
    "#create a webdriver object and set options for headless browsing\n",
    "options = Options()\n",
    "options.headless = True\n",
    "browser = webdriver.Chrome('./chromedriver',options=options)\n",
    "\n",
    "#uses webdriver object to execute javascript code and get dynamically loaded webcontent\n",
    "def get_js_soup(url,browser):\n",
    "    browser.get(url)\n",
    "    res_html = browser.execute_script('return document.body.innerHTML')\n",
    "    soup = BeautifulSoup(res_html,'html.parser') #beautiful soup object to be used for parsing html content\n",
    "    return soup\n",
    "\n",
    "def scrape_page(dir_url,browser):\n",
    "    links = []\n",
    "    #execute js on webpage to load listings on webpage and get ready to parse the loaded HTML \n",
    "    soup = BeautifulSoup(dir_url,'html.parser')\n",
    "    #soup = get_js_soup(dir_url,browser)\n",
    "    for link_holder in soup.find_all('a',class_='v-align-middle'):\n",
    "          try:\n",
    "              rel_link = link_holder['href'] #get url\n",
    "              links.append(rel_link)\n",
    "          except:\n",
    "              pass\n",
    "\n",
    "    return links\n",
    "\n",
    "links = []\n",
    "for page in range(1,101):\n",
    "    #dir_url = 'https://github.com/search?o=desc&p='+str(page)+'&q=react&s=stars&type=Repositories' #base url\n",
    "    dir_url = \"./html/\"+str(page)+\".html\"\n",
    "    ls = scrape_page(open(dir_url,'rb').read(),browser)\n",
    "    for l in ls:\n",
    "        links.append(l)\n",
    "urls_file = 'urls.txt'\n",
    "writew(links,urls_file)\n",
    "'''\n",
    "page = 1\n",
    "while page < 100:\n",
    "    url = 'https://github.com/search?o=desc&p='+str(page)+'&q=react&s=stars&type=Repositories'\n",
    "    print(page)\n",
    "    try:\n",
    "        webContent = urlopen(url).read()\n",
    "        ls = []\n",
    "        #execute js on webpage to load listings on webpage and get ready to parse the loaded HTML \n",
    "        soup = BeautifulSoup(webContent,'html.parser')\n",
    "        for link_holder in soup.find_all('a',class_='v-align-middle'):\n",
    "              try:\n",
    "                  rel_link = link_holder['href'] #get url\n",
    "                  ls.append('https://github.com'+rel_link)\n",
    "              except:\n",
    "                  pass\n",
    "        for l in ls:\n",
    "            links.append(l)\n",
    "        time.sleep(1)\n",
    "        page += 1     \n",
    "    except:\n",
    "        time.sleep(60+random.random())\n",
    "\n",
    "urls_file = 'urls.txt'\n",
    "writew(links,urls_file)\n",
    "'''\n",
    "\n",
    "'''\n",
    "#tidies extracted text \n",
    "def process_bio(bio):\n",
    "    bio = bio.encode('ascii',errors='ignore').decode('utf-8')       #removes non-ascii characters\n",
    "    bio = re.sub('\\s+',' ',bio)       #repalces repeated whitespace characters with single space\n",
    "    return bio\n",
    "\n",
    "def remove_script(soup):\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "    return soup\n",
    "\n",
    "\n",
    "#Checks if bio_url is valid \n",
    "def is_valid_homepage(bio_url,dir_url):\n",
    "    try:\n",
    "        ret_url = urllib.request.urlopen(bio_url).geturl() \n",
    "    except:\n",
    "        return False\n",
    "    urls = [re.sub('((https?://)|(www.))','',url) for url in [ret_url,dir_url]] #removes url scheme (https,http) or www \n",
    "    return not(urls[0]== urls[1])\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_faculty_page(url,browser):\n",
    "    packages = []\n",
    "    flag = False\n",
    "    soup = get_js_soup(url+'/blob/master/package.json',browser)\n",
    "    table = soup.find('table',class_='highlight tab-size js-file-line-container')\n",
    "    try:\n",
    "        for td in table.find_all('td',class_='blob-code blob-code-inner js-file-line'):\n",
    "            if flag == True and '}' in td.text.lower():\n",
    "                flag = False\n",
    "            if flag == True:\n",
    "                preparse = td.text\n",
    "                postparse = \"\"\n",
    "                isname = 0\n",
    "                for s in preparse:\n",
    "                    if s == '\"' and isname == 0:\n",
    "                        isname = 1\n",
    "                    elif s == '\"' and isname == 1:\n",
    "                        isname = 2\n",
    "                    elif isname == 1:\n",
    "                        postparse += s\n",
    "                packages.append(postparse)\n",
    "            if isinstance(td.text, str) and 'dependencies' in td.text.lower():\n",
    "                flag = True\n",
    "    except:\n",
    "        pass\n",
    "    return packages\n",
    "\n",
    "data = []\n",
    "for link in links:\n",
    "    package = scrape_faculty_page(link,browser)\n",
    "    if len(package) > 0:\n",
    "        data.append(package)\n",
    "\n",
    "datatxt = []\n",
    "for i in range(len(data)):\n",
    "    datastr = '['\n",
    "    for d in data[i]:\n",
    "        datastr += d + ','\n",
    "    datastr = datastr[:-1]\n",
    "    datastr += ']'\n",
    "    datatxt.append(datastr)\n",
    "\n",
    "    \n",
    "data_file = 'data.txt'\n",
    "writew(datatxt,data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
